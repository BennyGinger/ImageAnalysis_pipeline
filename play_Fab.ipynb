{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform mask into COSTO .json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from os import listdir, PathLike\n",
    "from os.path import join\n",
    "from pipeline.image_handeling.data_utility import load_stack\n",
    "from skimage.measure import find_contours, regionprops_table\n",
    "from numpyencoder import NumpyEncoder\n",
    "import json\n",
    "\n",
    "def mask_to_json(mask_fold_path: PathLike, save_path: PathLike, channel:str)->None:\n",
    "  \"\"\"Function that converts .tif labeled masks from np.array stacks into COCO .json format files.\n",
    "  Saves the file into the given location.\n",
    "  Args:\n",
    "      mask_fold_path (PathLike): Folder of the masks.\n",
    "      save_path (PathLike): Path where the .json is going to be saved. Must include the filename and the ending \".json\".\n",
    "      channel (string): The channel that should be transformed into .json.\n",
    "  Returns:\n",
    "      None, saves the .json into the given location.\"\"\"\n",
    "  img_path = []\n",
    "  for file in sorted(listdir(mask_fold_path)):\n",
    "      if channel in file:\n",
    "        img_path.append(join(mask_fold_path,file))\n",
    "  im_num = re.findall('f\\d+', str(img_path))\n",
    "  max_number = max(int(x[1:]) for x in im_num)\n",
    "  max_number\n",
    "\n",
    "  img_stack = load_stack(img_paths=img_path, channels=channel, frame_range=range(max_number), return_2D=True)\n",
    "\n",
    "  annotations_lst = []\n",
    "  images_lst = []\n",
    "  for frame, img in enumerate(img_stack):\n",
    "    outline_region =regionprops_table(img, properties=('label','bbox','area'))\n",
    "    frame_name = img_path[frame].rsplit('/',1)[1]\n",
    "    labels = outline_region['label']\n",
    "    bbox0 = outline_region['bbox-0']\n",
    "    bbox1 = outline_region['bbox-1']\n",
    "    bbox2 = outline_region['bbox-2']\n",
    "    bbox3 = outline_region['bbox-3']\n",
    "    area = outline_region['area']\n",
    "\n",
    "    for i in range(labels.shape[0]):\n",
    "        label = labels[i]\n",
    "        segmentation_lst=[]\n",
    "        regionmask=(img==label)\n",
    "        coords = find_contours(regionmask)\n",
    "        for y,x in coords[0]:\n",
    "            segmentation_lst.append(x+0.5)\n",
    "            segmentation_lst.append(y+0.5)\n",
    "        \n",
    "        annotations_lst.append({'id':label, \n",
    "                                'image_id':frame+1,\n",
    "                                'category_id':1,\n",
    "                                'segmentation':[segmentation_lst],\n",
    "                                'area':area[i],\n",
    "                                'bbox':[bbox0[i], bbox1[i], bbox2[i], bbox3[i]],\n",
    "                                'iscrowd': 0,\n",
    "                                'attributes': {'occluded': False}})\n",
    "\n",
    "    images_lst.append({'id': frame+1,\n",
    "              'width': img.shape[1],\n",
    "              'height': img.shape[0],\n",
    "              'file_name': frame_name,\n",
    "              'license': 0,\n",
    "              'flickr_url': '',\n",
    "              'coco_url': '',\n",
    "              'date_captured': 0})\n",
    "\n",
    "  json_dict = {'licenses': [{'name': '', 'id': 0, 'url': ''}],\n",
    "          'info': {'contributor': '',\n",
    "            'date_created': '',\n",
    "            'description': '',\n",
    "            'url': '',\n",
    "            'version': '',\n",
    "            'year': ''},\n",
    "          'categories': [{'id': 1, 'name': 'Cell', 'supercategory': ''}],\n",
    "          'images': images_lst,\n",
    "          'annotations':annotations_lst}\n",
    "\n",
    "  with open(save_path, \"w\") as file:\n",
    "      json.dump(json_dict, file, cls=NumpyEncoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint16)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "mask_folder = '/home/Fabian/ImageData/TrackingTestFiles/NeutrophilTrackingTest/mfap4-mpx_isohypo_2h_WT-MaxIP_s1/Masks_Cellpose'\n",
    "channel = 'GFP'\n",
    "savefile = '/home/Fabian/ImageData/Seriestest.json'\n",
    "\n",
    "mask_to_json(mask_fold_path=mask_folder, save_path=savefile, channel=channel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reconstruct mask based on COSTO .json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from skimage.draw import polygon2mask\n",
    "from os import PathLike\n",
    "\n",
    "def mask_from_json(path:PathLike)->tuple[list[str],np.array]:\n",
    "    \"\"\"Recreates an np.array like stack of masks based on an COCO .json file.\n",
    "    Return the np.array stack of masks and a list fo the Mask names.\n",
    "    Args:\n",
    "        path (PathLike): Path of the .json file to be loaded.\n",
    "    Returns:\n",
    "        tuple[list[str],np.array]: List of the image names, np.array containing the loaded masks\"\"\"\n",
    "    file = open(path)\n",
    "    json_file = json.load(file)\n",
    "\n",
    "    mask_lst = []\n",
    "    name_lst = []\n",
    "    width = json_file['images'][0]['width']\n",
    "    height = json_file['images'][0]['height']\n",
    "    annotations = json_file['annotations']\n",
    "    n_frame = len(json_file['images'])\n",
    "    mask = np.zeros((n_frame,width,height), dtype='int')\n",
    "    for images in json_file['images']:\n",
    "        name_lst.append(images['file_name'])\n",
    "    for annot in annotations:\n",
    "        segmentation = annot['segmentation'][0]\n",
    "        id = int(annot['id'])\n",
    "        frame_id = int(annot['image_id'])\n",
    "        coord_lst = []\n",
    "        for point in range(0, len(segmentation),2):\n",
    "            coord_lst.append((round(segmentation[point+1]), round(segmentation[point])))\n",
    "        tempmask =  polygon2mask((width,height),coord_lst)\n",
    "        tempmask[tempmask!=0] = id\n",
    "        mask[frame_id-1] = mask[frame_id-1] + tempmask\n",
    "    mask_lst.append(mask)\n",
    "    mask_stack = np.squeeze(np.stack(mask_lst))\n",
    "    return name_lst, mask_stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/Fabian/ImageData/instances_Series.json'\n",
    "name_lst, mask_stack = mask_from_json(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Play area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tifffile import imwrite\n",
    "from os import walk\n",
    "from os.path import join\n",
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = '/home/Fabian/ImageData/CalciumImmunCell/Macrophage/Laserwound/c1147x1382_003-MaxIP_s2/Masks_Cellpose'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnpicklingError",
     "evalue": "pickle data was truncated",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[68], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m files:\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m file\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.npy\u001b[39m\u001b[38;5;124m'\u001b[39m):            \n\u001b[0;32m----> 4\u001b[0m         npy \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_pickle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m      5\u001b[0m         filename \u001b[38;5;241m=\u001b[39m npy[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfilename\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      6\u001b[0m         mask \u001b[38;5;241m=\u001b[39m npy[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmasks\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m/opt/conda/envs/cp_dock/lib/python3.8/site-packages/numpy/lib/npyio.py:432\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    429\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m.\u001b[39mopen_memmap(file, mode\u001b[38;5;241m=\u001b[39mmmap_mode,\n\u001b[1;32m    430\u001b[0m                                   max_header_size\u001b[38;5;241m=\u001b[39mmax_header_size)\n\u001b[1;32m    431\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 432\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_pickle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_pickle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mpickle_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpickle_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mmax_header_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_header_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;66;03m# Try a pickle\u001b[39;00m\n\u001b[1;32m    437\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_pickle:\n",
      "File \u001b[0;32m/opt/conda/envs/cp_dock/lib/python3.8/site-packages/numpy/lib/format.py:793\u001b[0m, in \u001b[0;36mread_array\u001b[0;34m(fp, allow_pickle, pickle_kwargs, max_header_size)\u001b[0m\n\u001b[1;32m    791\u001b[0m     pickle_kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    792\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 793\u001b[0m     array \u001b[38;5;241m=\u001b[39m \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mUnicodeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    795\u001b[0m     \u001b[38;5;66;03m# Friendlier error message\u001b[39;00m\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mUnicodeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnpickling a python object failed: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    797\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou may need to pass the encoding= option \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    798\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto numpy.load\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (err,)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n",
      "\u001b[0;31mUnpicklingError\u001b[0m: pickle data was truncated"
     ]
    }
   ],
   "source": [
    "for root, dirs, files in walk(folder_path):\n",
    "    for file in files:\n",
    "        if file.endswith('.npy'):            \n",
    "            npy = np.load(join(root, file), allow_pickle=True).item()\n",
    "            filename = npy['filename']\n",
    "            mask = npy['masks']\n",
    "            imwrite(filename, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from tifffile import imread\n",
    "from matplotlib.pyplot import imshow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = '/home/Fabian/ImageData/CalciumImmunCell/Macrophage/Amputation/c1147x1382_001-MaxIP_s1/Masks_Cellpose/RFP_s01_f0001_z0001.tif'\n",
    "mask = imread(img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = '/home/Fabian/ImageData/CalciumImmunCell/Macrophage/Amputation/c1147x1382_001-MaxIP_s1/gnn_files/all_data_df.csv'\n",
    "df = pd.read_csv(csv_path)\n",
    "pytorch_path = '/home/Fabian/ImageData/CalciumImmunCell/Macrophage/Amputation/c1147x1382_001-MaxIP_s1/gnn_files/pytorch_geometric_data.pt'\n",
    "edge_index = torch.load(pytorch_path).edge_index\n",
    "output_path = '/home/Fabian/ImageData/CalciumImmunCell/Macrophage/Amputation/c1147x1382_001-MaxIP_s1/gnn_files/raw_output.pt'\n",
    "output_pred = torch.load(output_path)\n",
    "outputs = output_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_threshold = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_soft = torch.sigmoid(outputs)\n",
    "outputs_hard = (outputs_soft > decision_threshold).int()\n",
    "connected_indices = edge_index[:, outputs_hard.bool()]\n",
    "\n",
    "# ind_place = np.argwhere(connected_indices[0, :] == 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3817"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels, count = np.unique(connected_indices[1,:],return_counts=True)\n",
    "np.argmax(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2023, 2033])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "connected_indices[:,3817]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 20793])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "connected_indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition = connected_indices[1,:] == 2033"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "connected_indices = connected_indices[:,~condition]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([], dtype=int64),)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(connected_indices[1,:]==2033)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n"
     ]
    }
   ],
   "source": [
    "next_node_ind = -2\n",
    "mitosis = False\n",
    "if not mitosis and not next_node_ind==-1:\n",
    "    print('Hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([], dtype=int64),)\n",
      "tensor([2024, 2031])\n",
      "tensor([2025, 2032])\n",
      "tensor([2028, 2036])\n"
     ]
    }
   ],
   "source": [
    "print(np.where(connected_indices[1,:]==2033))\n",
    "print(connected_indices[:,3817])\n",
    "print(connected_indices[:,3820])\n",
    "print(connected_indices[:,3823])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(outputs_soft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_first_frame(cell_starts):\n",
    "    cols = [\"child_id\", \"parent_id\", \"start_frame\"]\n",
    "    df_parent = pd.DataFrame(index=range(len(cell_starts)), columns=cols)\n",
    "    df_parent.loc[:, [\"start_frame\", \"parent_id\"]] = 0\n",
    "    df_parent.loc[:, \"child_id\"] = cell_starts\n",
    "    return df_parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_in_specific_col(all_frames_traject, frame_ind, curr_node, next_node):\n",
    "    if curr_node in all_frames_traject[frame_ind, :]:\n",
    "        flag = 0\n",
    "        ind_place = np.argwhere(all_frames_traject[frame_ind, :] == curr_node)\n",
    "        if frame_ind + 1 < all_frames_traject.shape[0]:\n",
    "            all_frames_traject[frame_ind + 1, ind_place] = next_node\n",
    "    else:\n",
    "        flag = 1\n",
    "        ind_place = np.argwhere(all_frames_traject[frame_ind, :] == -2)\n",
    "        while ind_place.size == 0:\n",
    "            new_col = -2 * np.ones((all_frames_traject.shape[0], 1), dtype=all_frames_traject.dtype)\n",
    "            all_frames_traject = np.append(all_frames_traject, new_col, axis=1)\n",
    "            ind_place = np.argwhere(all_frames_traject[frame_ind, :] == -2)\n",
    "        ind_place = ind_place.min()\n",
    "        all_frames_traject[frame_ind, ind_place] = curr_node\n",
    "        if frame_ind + 1 < all_frames_traject.shape[0]:\n",
    "            all_frames_traject[frame_ind + 1, ind_place] = next_node\n",
    "    return flag, all_frames_traject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "connected_indices = edge_index[:, outputs_hard.bool()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    0,     1,     2,  ..., 12506, 12507, 12508])"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "connected_indices[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.array(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for array: array is 0-dimensional, but 1 were indexed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[206], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtest\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array: array is 0-dimensional, but 1 were indexed"
     ]
    }
   ],
   "source": [
    "test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_soft = torch.sigmoid(outputs)\n",
    "outputs_hard = (outputs_soft > decision_threshold).int()\n",
    "\n",
    "\n",
    "flag_id0_terminate = False\n",
    "# extract values from arguments\n",
    "connected_indices = edge_index[:, outputs_hard.bool()]\n",
    "\n",
    "\n",
    "# find number of frames for iterations\n",
    "frame_nums, counts = np.unique(df.frame_num, return_counts=True)\n",
    "all_frames_traject = np.zeros((frame_nums.shape[0], counts.max())) #crearing matrix with shape (rows=frames, column=max num of label in frame)\n",
    "\n",
    "# intialize the matrix with -2 meaning empty cell, -1 means end of trajectory,\n",
    "# other value means the number of node in the graph\n",
    "all_frames_traject[:, :] = -2\n",
    "all_trajectory_dict = {}\n",
    "str_track = ''\n",
    "df_parents = []\n",
    "\n",
    "for frame_ind in frame_nums: \n",
    "    nodes_indices = df[df.frame_num==frame_ind].index.values # find the places containing frame_ind\n",
    "    next_frame_indices = np.array([])\n",
    "    \n",
    "    if frame_ind == 0:  # for the first frame, we should fill the first row with node indices\n",
    "        all_frames_traject[frame_ind, :nodes_indices.shape[0]] = nodes_indices\n",
    "        df_parents.append(fill_first_frame(nodes_indices))\n",
    "    \n",
    "    num_starts = 0\n",
    "    cell_starts = []\n",
    "    \n",
    "    for i in nodes_indices:\n",
    "        if i in connected_indices[0, :]:\n",
    "            ind_place = np.argwhere(connected_indices[0, :] == i)\n",
    "            #check how many potential connections one cell has\n",
    "            if ind_place.shape[-1] > 1: # if more than one connection is possible:\n",
    "                next_frame_ind = connected_indices[1, ind_place].numpy().squeeze() #get the ID of the potential cells in the next frame\n",
    "            \n",
    "                next_frame = df.loc[next_frame_ind, [\"centroid_row\", \"centroid_col\"]].values #getting the centroid position for the potential connection points\n",
    "                curr_node = df.loc[i, [\"centroid_row\", \"centroid_col\"]].values #getting the original centroid\n",
    "\n",
    "                distance = np.sqrt(((next_frame - curr_node) ** 2).sum(axis=-1)) #get the euclidean distance between the node and the possible cells to connect\n",
    "                nearest_cell = np.argmin(distance, axis=-1) #get the index of the closest cell\n",
    "                # add to the array\n",
    "                next_node_ind = next_frame_ind[nearest_cell]\n",
    "\n",
    "            elif ind_place.shape[-1] == 1:  # one node in the next frame is connected to current node\n",
    "                next_node_ind = connected_indices[1, ind_place[0]]\n",
    "            else:  # no node in the next frame is connected to current node -\n",
    "                # in this case we end the trajectory\n",
    "                next_node_ind = -1\n",
    "        else:\n",
    "            # we dont find the current node in the edge indices matrix - meaning we dont have a connection\n",
    "            # for the node - in this case we end the trajectory and the cell\n",
    "            if i == 0:\n",
    "                flag_id0_terminate = True\n",
    "            next_node_ind = -1                \n",
    "        next_frame_indices = np.append(next_frame_indices, next_node_ind) #add the next node index or -1 for track stop into the next_frame_indices list\n",
    "        # count the number of starting trajectories\n",
    "        start, all_frames_traject = insert_in_specific_col(all_frames_traject, frame_ind, i, next_node_ind)\n",
    "        num_starts += start                \n",
    "        \n",
    "                \n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curr_node=array([565., 273.])\n",
      "distance=array([ 0.        , 66.09841148])\n",
      "nearest_cell=0\n",
      "next_node_ind=13\n",
      "next_frame_indices=array([13.])\n"
     ]
    }
   ],
   "source": [
    "frame_ind=0\n",
    "df_parents = []\n",
    "nodes_indices = df[df.frame_num==frame_ind].index.values # find the places containing frame_ind\n",
    "next_frame_indices = np.array([])\n",
    "\n",
    "if frame_ind == 0:  # for the first frame, we should fill the first row with node indices\n",
    "    all_frames_traject[frame_ind, :nodes_indices.shape[0]] = nodes_indices\n",
    "    df_parents.append(fill_first_frame(nodes_indices))\n",
    "\n",
    "\n",
    "num_starts = 0\n",
    "cell_starts = []\n",
    "\n",
    "i = nodes_indices[2]\n",
    "\n",
    "\n",
    "if i in connected_indices[0, :]:\n",
    "    ind_place = np.argwhere(connected_indices[0, :] == i)\n",
    "    if ind_place.shape[-1] > 1: # if more than one connection is possible:\n",
    "        next_frame_ind = connected_indices[1, ind_place].numpy().squeeze()\n",
    "        next_frame = df.loc[next_frame_ind, [\"centroid_row\", \"centroid_col\"]].values\n",
    "        curr_node = df.loc[i, [\"centroid_row\", \"centroid_col\"]].values\n",
    "        print(f'{curr_node=}')\n",
    "        distance = np.sqrt(((next_frame - curr_node) ** 2).sum(axis=-1))\n",
    "        print(f'{distance=}')\n",
    "        nearest_cell = np.argmin(distance, axis=-1)\n",
    "        print(f'{nearest_cell=}')\n",
    "        next_node_ind = next_frame_ind[nearest_cell]\n",
    "        print(f'{next_node_ind=}')\n",
    "next_frame_indices = np.append(next_frame_indices, next_node_ind)\n",
    "print(f'{next_frame_indices=}')\n",
    "# count the number of starting trajectories\n",
    "start, all_frames_traject = self.insert_in_specific_col(all_frames_traject, frame_ind, i, next_node_ind)\n",
    "num_starts += start    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_value(value, test_list):\n",
    "    print(value)\n",
    "    print(test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "blbla\n",
      "80\n",
      "blbla\n",
      "90\n",
      "blbla\n",
      "700\n",
      "blbla\n"
     ]
    }
   ],
   "source": [
    "unique_stuff = np.unique([90,80,700,50,80])\n",
    "test_list = 'blbla'\n",
    "partiaprint = partial(print_value, test_list=test_list)\n",
    "\n",
    "with ThreadPoolExecutor() as executer:\n",
    "    executer.map(partiaprint, np.unique([90,80,700,50,80]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_frames_traject_path = '/home/Fabian/ImageData/all_frames_traject.csv'\n",
    "all_frames_traject = np.genfromtxt(all_frames_traject_path, delimiter=',')\n",
    "cols = [\"child_id\", \"parent_id\", \"start_frame\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_ind = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "start1 = np.argwhere(all_frames_traject[frame_ind-1, :] == -1)\n",
    "start2 = np.argwhere(all_frames_traject[frame_ind-1, :] == -2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_node_ids = all_frames_traject[frame_ind, start1].squeeze(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23. 24. 25. 27. 29. 30.]\n"
     ]
    }
   ],
   "source": [
    "cell_starts = start_node_ids[start_node_ids>0]\n",
    "print(cell_starts)\n",
    "cell = 23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_dist = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "finish_node_ids = all_frames_traject[frame_ind - 1, ind_place].squeeze(axis=1)\n",
    "df_parent = pd.DataFrame(index=range(len(cell_starts)), columns=cols)\n",
    "df_parent.loc[:, \"start_frame\"] = frame_ind\n",
    "finish_cell = df.loc[finish_node_ids, [\"centroid_row\", \"centroid_col\"]].values\n",
    "curr_cell = df.loc[cell, [\"centroid_row\", \"centroid_col\"]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([16., 18., 14., 15., 19.])"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.delete(finish_node_ids,[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance = np.sqrt(((finish_cell - curr_cell) ** 2).sum(axis=-1)) #get the distance from every point\n",
    "distance_mask = distance < self.max_travel_dist #check that distances are inside the max_travel distance\n",
    "filtered_distance = distance[distance_mask] #apply the filter on the array\n",
    "if filtered_distance.size == 0:\n",
    "    df_parent.loc[ind, \"child_id\"] = cell\n",
    "    df_parent.loc[ind, \"parent_id\"] = 0\n",
    "    continue\n",
    "min_index = np.argmin(filtered_distance) #get the smalest distance index in the filtered array\n",
    "nearest_cell = np.where(distance_mask)[0][min_index] #get back the index from the original array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([16., 18., 14., 15., 19., 20.])"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finish_node_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 62.20128616, 120.42009799,  64.93843238, 180.62391868,\n",
       "       185.31055016, 233.85679379])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance = np.sqrt(((finish_cell - curr_cell) ** 2).sum(axis=-1))\n",
    "distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance = np.array([28,48,800,12,52,364,75])\n",
    "max_dist=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_array = distance < max_dist\n",
    "filtered = distance[masked_array]\n",
    "min_index = np.argmin(filtered)\n",
    "np.where(masked_array)[0][min_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered.size ==0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_array = np.argmin([17,5,7,9,10,18])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21.0"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_frames_traject[frame_ind, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1]])"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind_place = np.argwhere(connected_indices[0, :] == 1)\n",
    "ind_place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([12])"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_frame_ind = connected_indices[1, ind_place][0]#.numpy().squeeze()\n",
    "next_frame_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([358.07541105])"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_frame = df.loc[next_frame_ind, [\"centroid_row\", \"centroid_col\"]].values #getting the centroid position for the potential connection points\n",
    "curr_node = df.loc[2033, [\"centroid_row\", \"centroid_col\"]].values #getting the original centroid\n",
    "distance = np.sqrt(((next_frame - curr_node) ** 2).sum(axis=-1))\n",
    "distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_dist = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_array = distance < max_dist\n",
    "filtered = distance[masked_array]\n",
    "min_index = np.argmin(filtered)\n",
    "nearest_cell = np.where(masked_array)[0][min_index]\n",
    "next_node_ind = int(next_frame_ind[nearest_cell])\n",
    "next_node_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_travel_dist = 100\n",
    "\n",
    "\n",
    "def find_parent_cell(self, frame_ind, all_frames_traject, df, cell_starts):\n",
    "    ind_place = np.argwhere(all_frames_traject[frame_ind, :] == -1) #find all indeces were a track ended\n",
    "    finish_node_ids = all_frames_traject[frame_ind - 1, ind_place].squeeze(axis=1)# find the start IDs in the frame before\n",
    "    # print(f\"frame_ind: {frame_ind}, cell_starts: {cell_starts}, cell_ends: {finish_node_ids}\")\n",
    "\n",
    "    df_parent = pd.DataFrame(index=range(len(cell_starts)), columns=self.cols)\n",
    "    df_parent.loc[:, \"start_frame\"] = frame_ind\n",
    "\n",
    "    if finish_node_ids.shape[0] != 0:\n",
    "        if self.is_3d:\n",
    "            finish_cell = df.loc[finish_node_ids, [\"centroid_depth\", \"centroid_row\", \"centroid_col\"]].values\n",
    "        else:\n",
    "            finish_cell = df.loc[finish_node_ids, [\"centroid_row\", \"centroid_col\"]].values\n",
    "        for ind, cell in enumerate(cell_starts):\n",
    "            if self.is_3d:\n",
    "                curr_cell = df.loc[cell, [\"centroid_depth\", \"centroid_row\", \"centroid_col\"]].values\n",
    "            else:\n",
    "                curr_cell = df.loc[cell, [\"centroid_row\", \"centroid_col\"]].values\n",
    "\n",
    "            distance = ((finish_cell - curr_cell) ** 2).sum(axis=-1)\n",
    "            nearest_cell = np.argmin(distance, axis=-1)\n",
    "            parent_cell = int(finish_node_ids[nearest_cell])\n",
    "            df_parent.loc[ind, \"child_id\"] = cell\n",
    "            df_parent.loc[ind, \"parent_id\"] = parent_cell\n",
    "    else:\n",
    "        df_parent.loc[:, \"child_id\"] = cell_starts\n",
    "        df_parent.loc[:, \"parent_id\"] = 0\n",
    "\n",
    "    return df_parent"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cp_dock",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
